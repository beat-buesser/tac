# The FEAST project

**Name:** Feast - Feature Store for Machine Learning

**Homepage:** https://feast.dev/

**GitHub:** https://github.com/feast-dev/feast

## Project idea #1: Real-time Observability

**Time: **350 hr = 8 weeks

**Potential Mentors:**
Oleksii, Felix, Willem

**Prerequisites:** 

* Python
* Go

**Nice to have:**
Familiarity with cloud infrastructure and monitoring tools (Kubernetes, Statsd, Grafana, Prometheus)

**Difficulty level:** moderate

**Ideal candidate:** advanced or student level both OK

## Description
Feast is an open source feature store that enables users to register data sources and features, discover features at training time, and reliably serve features in online systems. Feast uses compute systems (primarily data warehouses) to do large scale joins. Feast also provides the ability for users to stream real-time features into an online store, from which production ML systems read feature values in order to make predictions.

Despite the fact that Feast operates as a production data system for operational machine learning, Feast is still relatively limited in the types of monitoring it allows. Teams that are running Feast at scale need a way to understand how the system is operating. This project seeks to add support for end-to-end monitoring to Feast, which will allow operators of the system to get visibility into component level metrics at ingestion, storage, compute, and retrieval time, as well as data level metrics to understand which features are available within the feature store.

This project aims to add real-time observability into the following milestones:

* RFC for overall design (approved by community + core Feast maintainers)
* Implementation (including tests + documentation + tutorial)
* Metric collection at the following points using Statsd or equivalent
* Materialization library (writes to online store)
* Python/Go Online Servers (reads from online store)
* Push API (writes to online store)
* Feature Transformation Server (computes features)
* Metric sink (StatsD)
* Configuration of metric collection through feature_store.yaml
* Create a Helm chart for deployment of the metric stack
* (optional) Build a data visualization dashboard in Grafana to trend metrics in Prometheus
* (optional): Add support for sinking not only performance metrics but also feature values to Prometheus. This allows operators to set alerts based on both metrics and feature values.
* (optional): Add support for updating Prometheus/Grafana configuration based on the feature views in the registry.
* (optional): Make it possible for users to detect anomalies in data by allowing them to configure alerts based on tags in feature views

## Project idea #2: Batch transformations
Time: 350 hr = 8 weeks
Potential Mentors:
Achal, Tsotne, Felix
Prerequisites:
Python
SQL
Nice to have: familiarity with MLOps / ML tooling (Spark, dbt, dagster, Dataflow, Flink, etc)
Difficulty level: moderate
Ideal candidate: advanced or student level both OK
Description:
Feast is an open source feature store that enables users to register data sources and features, discover features at training time, and reliably serve features in online systems. Feast uses compute systems (primarily data warehouses) to do large scale joins.
One capability Feast is missing is the ability to support batch transformations (i.e. transforming large amounts of time series data into feature values for generating training data). Many users want a light-weight way to do feature engineering in Feast. Many other users have existing transformation pipelines (that data scientists use), but want to use Feast as a way of encouraging feature / data pipeline re-use.
This project aims to add feature engineering in several milestones:
RFC for overall design (approved by community + core Feast maintainers)
Implementation (including tests + documentation + tutorial)
SQL based transformation (leveraging the existing data warehouses registered as offline stores) defined as part of a Feast BatchFeatureView that uses the SQL dialect of the underlying offline store
Pandas based transformations (using Ray/Dask) for a pythonic transformation
e.g. data scientists can easily transition from notebook driven development
Integration into Feast Web UI
(optional): using pandas based batch transformations to execute consistent transformations at training vs serving time (ODFV)
(optional): allow Spark based transformations (spark_sql vs pyspark)
(optional): allow dbt based transformations
(optional): integration with existing transformation pipelines (dbt, Spark, Dataflow)
Project idea #3: Streaming Compute
Time: 350 hr = 8 weeks
Potential Mentors:
Oleksii, Danny, Achal
Prerequisites:
Java or Scala
Nice to have: familiarity stream processing frameworks (Apache Spark, Apache Beam, Apache Flink)
Difficulty level: moderate
Ideal candidate: advanced or student level both OK
Description:
Feast is an open source feature store that enables users to register data sources and features, discover features at training time, and reliably serve features in online systems. Feast uses compute systems (primarily data warehouses) to do large scale joins. Feast also provides the ability for users to stream real-time features into an online store, from which production ML systems read feature values in order to make predictions.
The “streaming” implementation that Feast has today is very limited. It’s simply a push-based API to which users of Feast should write their values. Essentially Feast requires users to launch and manage their own streaming jobs, compute their own feature values based on streaming events, and then push them to Feast through this push API.
The goal of this project is to build a real-time streaming compute stack that makes it possible for users to define streaming feature aggregations, and have this streaming system automatically launch jobs to compute features and write them to the feature store.
This project aims to add real-time observability into the following milestones:
RFC for overall design (approved by community + core Feast maintainers)
Implementation (including tests + documentation + tutorial)
Develop a streaming job launcher for a single platform (Spark, Flink, Beam, etc)
Develop a streaming job orchestrator to ensure jobs are started, stopped, or restarted.
Develop feature definitions (feature views) that allow users to define aggregations and register the with Feast
Add support for at least a single streaming source (like Kafka)
(optional) Integrate metric production with the streaming stack
(optional) Allow users to develop and test streaming jobs from within a local development environment
(optional) Add support for partial aggregations/tiling, which lets the streaming compute system aggregate feature values partially, while the online serving stack takes care of the complete aggregation
The Milvus project
Name: Milvus Vector Database
Homepage: https://milvus.io/
GitHub: https://github.com/milvus-io/milvus

Milvus was created in 2019 with a singular goal: store, index, and manage massive embedding vectors generated by deep neural networks and other machine learning (ML) models.

As a database specifically designed to handle queries over input vectors, it is capable of indexing vectors on a trillion scale. Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from unstructured data.
Project idea #1: Support Range Search in Milvus
Abstract

Range search is an operator returning all elements that are within a given radius of the query point. It is another widely used similarity search operator. This project will focus on integrating range search functionality into Milvus.

Difficulty
Time
Priority


Mentors
Hard
350hr
High


YH Mo https://github.com/yhmo



Technical Details

The underlying vector search library, such as Faiss, may implement range search functionalities already, while others like HNSW are not. To support range search in knowhere, we will need to:

1) implement range search on HNSW(optional)

2) implement  range search  unit test and ci test for Milvus's execution engine - knowhere

3) add a range search API for Milvus

4) implement the range iteration logic 

Helpful Experience
Proficient in Go and Cpp
Ability to read scientific publications
Nice to have: knowledge of vector index, Knowledge of distributed system
First Steps
Learn what range search is, and the basic idea of Faiss and HNSW
Setup development environment of Knowhere and Milvus
Read the code of HNSW, figure out how to support range search
Discuss your proposal with the mentors and on community tech meeting
Join our slack channel or mail our mentors to get in touch with them for more information


Project idea #2: Distributed file system Support for Milvus 

Abstract

Milvus adopt the design principle of computation storage disaggregation. The primary storage we support is object storage such as Minio && S3. This project will focus on integrating Milvus with HDFS,  and probably other distributed file systems such as Ceph and GlusterFS

Difficulty
Time
Priority
Mentors
Easy
175hr
Medium
Congqi Xia https://github.com/congqixia



Technical Details

Milvus already did a great job on storage abstraction. We can support both local disk and Minio as chunk storage. To support DFS as the storage backend, we will need to:

1.Implement DFSChunkManager

2.finish all the unit test and integration test together with distributed file system.

3.Add monitoring metrics about IOPs, traffic and latency.

4.performance benchmark

Helpful Experience

Proficient in Go
Nice to have: have the experience of using distributed storage, familiar with a distributed system.
First Steps

Read the document and API usage of HDFS
Read the code in Milvus about MinioChunkManager
Discuss your proposal with the mentors.
Join our slack channel or mail our mentors to get in touch with them for more information
Project idea #3: Implement  auto scaler for milvus

Abstract

Milvus is a cloud native vector database and we have very good elasticity thanks to the design of storage computation disaggregation. In this project we want to support a cluster auto scalar which could change the cluster size based on user traffic.

Difficulty
Time
Priority
Mentors
Moderate
175hr
Medium
Edward Zeng
https://github.com/LoveEachDay



Technical Details

The key to this project is to find some open source auto scalar and build on top of it. The task owner must have deep knowledge about what metrics can be used as the judgement to expand and shrink the cluster. To implement a stable auto scalar, follow the steps:

Follow the benchmark of milvus, do some stress tests and find the correct metrics that indicate the system load(May need to add actual metrics)
Implement the logic to collect all the metrics
Implement the core logic about making operation decision。
Call Milvus K8s operator to expand or shrink the cluster.
Do more benchmark tests with various workloads, test if the auto scalar works as expected.
Helpful Experience

Proficient in Go
Knowledge about K8s
Basic knowledge K8s auto scalar and Knative, familiar with prometheus.
First Steps

Investigate different auto scalars
Do performance benchmark and understand all metrics on prometheus cluster
Design the core logic of resource orchestration
Discuss your proposal with the mentors.
Join our slack channel or mail our mentors to get in touch with them for more information
